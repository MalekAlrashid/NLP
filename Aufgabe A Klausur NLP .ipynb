{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work space vorbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importieren unsere Bibliotheken wie NLTK, Codecs und HanoverTagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs \n",
    "import nltk\n",
    "from HanTa import HanoverTagger as ht\n",
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "with codecs.open('woerter/NN_masc.txt','r','utf8') as datei:\n",
    "    for word in datei:\n",
    "        words.append((word.strip(),'m'))# m für Maskulin der\n",
    "with codecs.open('woerter/NN_fem.txt','r','utf8') as datei:\n",
    "    for word in datei:\n",
    "        words.append((word.strip(),'f'))# f für Femenin die \n",
    "with codecs.open('woerter/NN_neutr.txt','r','utf8') as datei:\n",
    "    for word in datei:\n",
    "        words.append((word.strip(),'n'))# n für Neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['##n', '#nu', 'nut', 'utz', 'tze', 'zer', 'er#', 'r##'], 'm'),\n",
       " (['##v',\n",
       "   '#ve',\n",
       "   'ver',\n",
       "   'ert',\n",
       "   'rte',\n",
       "   'tei',\n",
       "   'eid',\n",
       "   'idi',\n",
       "   'dig',\n",
       "   'igu',\n",
       "   'gun',\n",
       "   'ung',\n",
       "   'ngs',\n",
       "   'gsb',\n",
       "   'sbu',\n",
       "   'bud',\n",
       "   'udg',\n",
       "   'dge',\n",
       "   'get',\n",
       "   'et#',\n",
       "   't##'],\n",
       "  'n')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8616"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punkt 3    Trigramme extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bevor ich meine Datein für Test und Training Daten trenne, wollte ich zuerst den Punkt 3(Trigramme extrahieren) erledigen, damit ich vom Anfang die Wörter ausrichten kann.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(string,n):\n",
    "    liste = []\n",
    "    if n < len(string):\n",
    "        for p in range(len(string) - n + 1) :\n",
    "            tg = string[p:p+n]\n",
    "            liste.append(tg)\n",
    "    return liste\n",
    "def xgram(string):\n",
    "    return [w for w in ngram(string.lower(),3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punkt 2    Wörter erweitern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wir mussen den Wörtern Sonderzeichnen einfügen, damit wir gute Ergebnisse bei den Trigramme bekommen. In folgendem Code extrahieren wir die trigramme und erweitern wir die Wörte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[(xgram('##'+a+'##'),b) for (a,b) in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['##k', '#ko', 'kon', 'onz', 'nze', 'zer', 'ern', 'rn#', 'n##'], 'm')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punkt 1 : Test und Training Daten trennen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser Daten sin ca. 8600 Datei. wir nehmen für die Training 80% davon. Mit den Anderen testen wir unser Model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(words)\n",
    "train_data = words[:6880]\n",
    "test_data = words[6880:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anteil der Wörter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch die  Funktion (features_from_text) können wir wisse,was der Anteil eines Wortes/ eines Trigramms ist. Das können entdecken, wenn wir den Anzahl des Wortes durch den Anzahl aller Textwörter teilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def features_from_text(text):\n",
    "    wordcounts = Counter()\n",
    "    tlen = 0\n",
    "    wordcounts.update(text)\n",
    "    tlen +=len(text)\n",
    "    return {w:wordcounts[w]/tlen for w in wordcounts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poslist = [\"NN\",\"NE\"]\n",
    "test_data= [(features_from_text(text), cl)for text,cl in test_data]\n",
    "train_data= [(features_from_text(tex,poslist),cl) for text,cl in train_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Häufigkeit aller vorkommenden Trigramme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir die Trigramme, die häufig in der Datein vorkommen, entdecken zu können, bauen wir ein Counter und zahlen wir, wie oft ein Wort bzw. ein Trigramm kommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t##', 1162),\n",
       " ('g##', 1065),\n",
       " ('ung', 1058),\n",
       " ('e##', 1056),\n",
       " ('r##', 941),\n",
       " ('ng#', 903),\n",
       " ('er#', 746),\n",
       " ('##s', 728),\n",
       " ('sch', 712),\n",
       " ('n##', 687),\n",
       " ('##a', 555),\n",
       " ('##b', 520),\n",
       " ('##k', 448),\n",
       " ('ter', 408),\n",
       " ('ver', 407),\n",
       " ('##g', 398),\n",
       " ('eit', 371),\n",
       " ('##f', 368),\n",
       " ('##v', 367),\n",
       " ('##m', 357)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docfreq = Counter()\n",
    "for (wfreq,c) in train_data:\n",
    "    docfreq.update(wfreq.keys())\n",
    "docfreq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einbauen des Trainingsmodel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punkt 4 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Importieren die Bibioltek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Die häufigste vorkommende Trigramme rausnehmen, damit wir den Netz trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "\n",
    "\n",
    "allfeatures = [w for w in docfreq if docfreq[w] > 4]\n",
    "\n",
    "def make_feat_vec(featmap,featlist):\n",
    "    vec = []\n",
    "    for f in featlist:\n",
    "        vec.append(featmap.get(f,0.0))\n",
    "    return vec\n",
    "\n",
    "train_vec =  [make_feat_vec(feats,allfeatures) for feats,cls in train_data]\n",
    "train_label = [cls for feats,cls in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - fitten wir das Modell durch die Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000000.0, max_iter=4000, verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = linear_model.LogisticRegression(C=1e9, verbose = True, max_iter=4000 )\n",
    "logreg.fit(train_vec,train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Testen wir das Model mit den Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = [make_feat_vec(feats,allfeatures) for feats,cls in test_data]\n",
    "test_label = [cls for feats,cls in test_data]\n",
    "\n",
    "pred_label = list(logreg.predict(test_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punkt 5 :Bewertung des Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir evaluieren das Model und wiessen, wie es correct ist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.8 Prozent korrekt\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(test_label)):\n",
    "    if test_label[i] == pred_label[i]:\n",
    "        correct+=1\n",
    "print(\"{0:.1f} Prozent korrekt\".format(100* float(correct)/len(test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ried"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
